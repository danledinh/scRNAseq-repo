{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a876e273-a3fd-4716-b102-245cd719f54a",
   "metadata": {},
   "source": [
    "# Processing and Filtering UMI Expression Matrices\n",
    "\n",
    "Note that we needed 128 GB of RAM to run this notebook and that the notebook was run the environment created by scanpy-default-mamba.yml\n",
    "\n",
    "**ALSO NOTE: If you change any samples in this process, it will change the barcodes selected due to the method for setting seeds (we set seeds prior to the whole sampling not prior to each sample, so any changes in the samples, will change downstream results). Similarly, you cannot only run a subsample of the data, you must run the whole thing to prevent similar issues**\n",
    "\n",
    "Lastly, all paths have been scrubbed from this notebook, so please insert relevant paths to files if you wish to rerun (search for '#INSERT HERE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "805d63f8-0968-4465-bd0c-615693407724",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scanpy.external as sce\n",
    "import seaborn as sns\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from collections.abc import Iterable \n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8415a124-be16-4b4b-98ad-84ad98f96a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Specify Analysis Parameters\n",
    "GENE_PCT=0.1\n",
    "SEED=42\n",
    "N_PLATFORMS_DEVIANT = 1\n",
    "DOUBLET_THRESH=0.25\n",
    "FEATURE_SELECT_GENE_N = 2000\n",
    "run_scrublet=False\n",
    "filter_cells=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad3b0f3-1093-4411-9b11-a9abe2df59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir=##INSERT HERE\n",
    "figure_dir=##INSERT HERE\n",
    "supplement_dir=os.path.join(figure_dir, 'supplement')\n",
    "supplement_fig_dir=os.path.join(supplement_dir, 'figures')\n",
    "supplement_table_dir=os.path.join(supplement_dir, 'tables')\n",
    "\n",
    "##make figure directories if they doesn't exist\n",
    "os.makedirs(parent_dir, exist_ok=True)\n",
    "os.makedirs(figure_dir, exist_ok=True)\n",
    "os.makedirs(supplement_fig_dir, exist_ok=True)\n",
    "os.makedirs(supplement_table_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142c34fd-d44a-4d71-9bb1-98c30991595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LOAD CUSTOM FUNCTIONS FROM FUNCTIONS DIRECTORY IN REPO\n",
    "\n",
    "##Note: These function files load their own necessary packages, so we don't need to load these dependencies here\n",
    "\n",
    "##define function location\n",
    "functions_dir = ##INSERT HERE\n",
    "\n",
    "##if the directory is not in path, add\n",
    "if functions_dir not in sys.path:\n",
    "    sys.path.append(functions_dir)\n",
    "\n",
    "##import functions\n",
    "from get_statistics_functions import load_AnnData, load_raw_AnnData ##import all relevant processing functions\n",
    "import highly_deviant_genes as hdg ##import all relevant highly deviant gene detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa0bed5c-b543-48b6-9979-84841b209304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define File Names\n",
    "data_dir=##INSERT HERE\n",
    "\n",
    "##filtered files (for barcode matching and labeling of called cells)\n",
    "filtered_files = {'10X_3-rep1': ##INSERT HERE,\n",
    "                  '10X_3-rep2': ##INSERT HERE,\n",
    "                  '10X_5-rep1': ##INSERT HERE,\n",
    "                  '10X_5-rep2': ##INSERT HERE,\n",
    "                  '10X_FRP-rep1': ##INSERT HERE, \n",
    "                  '10X_FRP-rep2': ##INSERT HERE, \n",
    "                  'BD-rep1': ##INSERT HERE, \n",
    "                  'BD-rep2': ##INSERT HERE, \n",
    "                  'Fluent-rep1': ##INSERT HERE,\n",
    "                  'Fluent-rep2': ##INSERT HERE,\n",
    "                  'Fluent-rep3':  ##INSERT HERE,\n",
    "                  'Honeycomb-rep1':  ##INSERT HERE,\n",
    "                  'Honeycomb-rep2':  ##INSERT HERE,\n",
    "                  'Singleron-rep1':  ##INSERT HERE,\n",
    "                  'Singleron-rep2':  ##INSERT HERE,\n",
    "                  'Scipio-rep1':  ##INSERT HERE,\n",
    "                  'Scipio-rep2':  ##INSERT HERE,\n",
    "                  'Parse-rep1':  ##INSERT HERE,\n",
    "                  'Scale-rep1':  ##INSERT HERE,\n",
    "                  'Broad-Reference':  ##INSERT HERE,\n",
    "                 }\n",
    "\n",
    "##get full paths\n",
    "filtered_files = {method:os.path.join(data_dir, file) for method, file in filtered_files.items()}\n",
    "\n",
    "##define methods\n",
    "methods = filtered_files.keys()\n",
    "\n",
    "##get tags\n",
    "platform_tags, rep_tags = zip(*[mr.split(\"-\") for mr in methods]) #split on the hyphen and separate into two lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dbf11-2409-4316-be60-0527f629cbd4",
   "metadata": {},
   "source": [
    "Get some summary stats for all platforms to make downsampling easier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9ece9d-1ffd-4fb3-ac3b-def16e84e1e2",
   "metadata": {},
   "source": [
    "#### <center> Process Data Separately (with Reference), so we can compare cell annotation across platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219b47f-f2ac-4002-8b77-14aee01728f7",
   "metadata": {},
   "source": [
    "Note that we will add the Broad Institutue Multiplatform Dataset (converted from Seurat) as the 'gold standard' for comparison AND for integration and label transfer (https://singlecell.broadinstitute.org/single_cell/study/SCP424/single-cell-comparison-pbmc-data and https://github.com/satijalab/seurat-data/). We also split the Broad reference to get a validation set (same size as other platforms) that we can use to assess how good our cell annotation methods are later. \n",
    "\n",
    "To make sure we are being consistent with our analysis, we will first subset our validation dataset to 10000 cells to approximate the cell number target we used for each platform. We will then identify \"doublets\" with scanpy's external link to Scrublet which simulates doublets by randomly combining two observations into one and comparing the observed results with the simulated results. We manually set a threshold of 0.25 for the simulated doublets because the automatic caller gives poor results and 0.25 seems to be the most consistent across platforms. \n",
    "\n",
    "To be fair for downstream analysis, we will downsample again to the smallest number of non-doublet cells for all platforms (except the reference) and process from there.\n",
    "\n",
    "Lastly, we will find deviant genes per platform (top 2000) that can be used for PCA and cell annotation later\n",
    "\n",
    "Note that we perform batch normalization on the combined reference-platform anndata object so that we normalize according to each library size rather than overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9418f71e-a92b-4ed2-a911-c69467e846b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10X_3-rep1 ...\n",
      "Loading 10X_3-rep2 ...\n",
      "Loading 10X_5-rep1 ...\n",
      "Loading 10X_5-rep2 ...\n",
      "Loading 10X_FRP-rep1 ...\n",
      "Loading 10X_FRP-rep2 ...\n",
      "Loading BD-rep1 ...\n",
      "Loading BD-rep2 ...\n",
      "Loading Fluent-rep1 ...\n",
      "Loading Fluent-rep2 ...\n",
      "Loading Fluent-rep3 ...\n",
      "Loading Honeycomb-rep1 ...\n",
      "Loading Honeycomb-rep2 ...\n",
      "Loading Parse-rep2 ...\n",
      "Loading Scale-rep1 ...\n",
      "Loading Scipio-rep1 ...\n",
      "Loading Scipio-rep2 ...\n",
      "Loading Singleron-rep1 ...\n",
      "Loading Singleron-rep2 ...\n",
      "Loading Broad-Reference-Val ...\n",
      "Loading Broad-Reference ...\n"
     ]
    }
   ],
   "source": [
    "##Setup Directory to save anndata objects\n",
    "anndata_object_dir = os.path.join(parent_dir, 'anndata_objects')\n",
    "os.makedirs(anndata_object_dir, exist_ok=True)\n",
    "\n",
    "##Load in all data and perform scrublet\n",
    "\n",
    "##Set filtering params for pre-scrublet filtering\n",
    "##Cells with less than 5 genes probably aren't cells and genes not found in at least 3 cells probably aren't useful\n",
    "##Based on ATAC-seq guidelines\n",
    "min_genes=5\n",
    "min_cells=3\n",
    "n_doublets={}\n",
    "doublet_rates={}\n",
    "doublet_dir = os.path.join(parent_dir, 'doublet_data')\n",
    "os.makedirs(doublet_dir, exist_ok=True)\n",
    "\n",
    "if run_scrublet:\n",
    "\n",
    "    ##initialize data lists and dicts\n",
    "    adata_all = {}\n",
    "\n",
    "    ##load data and process for cell selection\n",
    "    for method, file in filtered_files.items():\n",
    "        \n",
    "        print(f\"Method: {method}\")\n",
    "\n",
    "        ## load adata file depending on filename\n",
    "        if re.match(r'.*10X.*', file):\n",
    "            adata = load_raw_AnnData(file)\n",
    "        else:\n",
    "            adata = load_AnnData(file)\n",
    "\n",
    "        ##create stats dictionary in data to store n_doublet and doublet rate\n",
    "        if 'stats' not in adata.uns:\n",
    "            adata.uns['stats'] = {}\n",
    "\n",
    "        ##update obs names if and only if it hasn't been tagged with method tag\n",
    "        if method not in adata.obs_names[0]:\n",
    "            adata.obs_names = [f'{bc}_{method}' for bc in adata.obs_names]\n",
    "\n",
    "        # filter barcodes with few genes genes or counts so they don't affect scrublet\n",
    "        sc.pp.filter_cells(adata, min_genes = min_genes) ##5 is default because used in scATAQ-seq paper\n",
    "\n",
    "        ##if reference, split into reference and validation\n",
    "        if method == 'Broad-Reference':\n",
    "            \n",
    "            ##for some reason the raw data contains '_index' as a column which gives an error when saving, so fix with this line\n",
    "            adata.__dict__['_raw'].__dict__['_var'] = adata.__dict__['_raw'].__dict__['_var'].rename(columns={'_index': 'features'})\n",
    "            \n",
    "            \n",
    "            cell_index = adata.obs_names ##all indices of ref\n",
    "            N_test = 10000 ##initial target for all platforms\n",
    "            np.random.seed(SEED) ##reset seed to make sure it will Always be the same split even if done outside of loop\n",
    "            test_ind = np.random.choice(cell_index, N_test, replace = False) ##select test cell indices to be same size as other platforms\n",
    "            train_ind = np.setdiff1d(cell_index, test_ind) ##get train indices as those not in test\n",
    "\n",
    "            ##subset test data as adata_val\n",
    "            adata_val = adata[test_ind]\n",
    "\n",
    "            ##subset train data as adata\n",
    "            adata = adata[train_ind]\n",
    "\n",
    "            ##Filter genes so they don't affect scrblet\n",
    "            sc.pp.filter_genes(adata_val, min_cells = min_cells)##3 is default because used in scATAQ-seq paper\n",
    "\n",
    "            ##Identify and remove doublets with scrublet (Manually setting doublet threshold to 0.25 because it gives best results across platforms)\n",
    "            sce.pp.scrublet(adata_val, threshold=DOUBLET_THRESH) ##identify doublet with threshold\n",
    "            sce.pl.scrublet_score_distribution(adata_val)\n",
    "            adata_val.uns['stats']['n_doublet'] = np.sum(adata_val.obs['predicted_doublet'] == True) ##get number of doublets identified\n",
    "            adata_val.uns['stats']['doublet_rate'] = adata_val.uns['stats']['n_doublet']/adata_val.shape[0] ##get doublet rate\n",
    "            \n",
    "            ##Add platform info\n",
    "            adata_val.obs['platform'] = 'Broad-Reference-Val'\n",
    "            adata_val.obs['platform_type'] = 'reference_Broad_Val'\n",
    "            \n",
    "            ##save adata object with doublet scores prior to doublet removal\n",
    "            output_file = os.path.join(anndata_object_dir, f'Broad-Reference-Val_filtered_withdoublets.h5ad')\n",
    "            adata_val.write_h5ad(output_file)\n",
    "            \n",
    "            ##remove doublets\n",
    "            adata_val = adata_val[adata_val.obs['predicted_doublet'] == False] ##keep only singlets\n",
    "            \n",
    "            n_doublets['Broad-Reference-Val'] = adata_val.uns['stats']['n_doublet']\n",
    "            doublet_rates['Broad-Reference-Val'] = adata_val.uns['stats']['doublet_rate']\n",
    "\n",
    "\n",
    "            ##Add \"-Val\" tag to the end of obs_names so that it is tagged with \"Broad-Reference-Val\" instead of \"Broad-Reference\"\n",
    "            adata_val.obs_names = [f'{bc}-Val' for bc in adata_val.obs_names]\n",
    "\n",
    "            \n",
    "\n",
    "        #Filter genes so they don't affect scrblet\n",
    "        sc.pp.filter_genes(adata, min_cells = min_cells) ##3 is default because used in scATAQ-seq paper\n",
    "\n",
    "        ##Identify and remove doublets with scrublet (Manually setting doublet threshold to 0.25 because it gives best results across platforms)\n",
    "        sce.pp.scrublet(adata, threshold=DOUBLET_THRESH) ##identify doublet with threshold\n",
    "        sce.pl.scrublet_score_distribution(adata)\n",
    "        adata.uns['stats']['n_doublet'] = np.sum(adata.obs['predicted_doublet'] == True) ##get number of doublets identified\n",
    "        adata.uns['stats']['doublet_rate'] = adata.uns['stats']['n_doublet']/adata.shape[0] ##get doublet rate\n",
    "\n",
    "        ## Add platform names to adata.obs\n",
    "        adata.obs['platform'] = method\n",
    "\n",
    "        # Add platform type\n",
    "        if method in ['BD-rep1','BD-rep2', 'Honeycomb-rep1','Honeycomb-rep2','Singleron-rep1', 'Singleron-rep2']:\n",
    "            adata.obs['platform_type'] = 'well-based'\n",
    "        elif method in ['Scale-rep1', 'Scale-rep2', 'Parse-rep1', 'Parse-rep2']:\n",
    "            adata.obs['platform_type'] = 'combinatorial'\n",
    "        elif method in ['10X_3-rep1', '10X_3-rep2', '10X_5-rep1', '10X_5-rep2','10X_FRP-rep1','10X_FRP-rep2']:\n",
    "            adata.obs['platform_type'] = 'droplet_GEMs'\n",
    "        elif method in ['Fluent-rep1', 'Fluent-rep2', 'Fluent-rep3']:\n",
    "            adata.obs['platform_type'] = 'droplet_PIPs'\n",
    "        elif method in ['Scipio-rep1', 'Scipio-rep2']:\n",
    "            adata.obs['platform_type'] = 'hydrogel'\n",
    "        elif method == 'Broad-Reference':\n",
    "            adata.obs['platform_type'] = \"reference_Broad\"\n",
    "            \n",
    "        ##save adata object with doublet scores prior to doublet removal\n",
    "        output_file = os.path.join(anndata_object_dir, f'{method}_filtered_withdoublets.h5ad')\n",
    "        adata.write_h5ad(output_file)\n",
    "            \n",
    "        ##remove doublets for downstream steps\n",
    "        adata = adata[adata.obs['predicted_doublet'] == False] ##keep only singlets\n",
    "        \n",
    "        n_doublets[method] = adata.uns['stats']['n_doublet']\n",
    "        doublet_rates[method] = adata.uns['stats']['doublet_rate']\n",
    "\n",
    "        ##store all data and all genes\n",
    "        adata_all[method] = adata\n",
    "\n",
    "        del adata\n",
    "\n",
    "    ##add adata_val to adata_all\n",
    "    adata_all['Broad-Reference-Val'] = adata_val\n",
    "\n",
    "    ##remove for memory\n",
    "    del adata_val\n",
    "    \n",
    "    doublet_df = pd.DataFrame([n_doublets, doublet_rates]).T\n",
    "    doublet_df = doublet_df.reset_index()\n",
    "    doublet_df.columns = ['method', 'n_doublets', 'doublet_rate']\n",
    "    doublet_df.to_csv(os.path.join(doublet_dir, 'doublet_summary_df.csv'), index=False)\n",
    "    \n",
    "    ##also save to supplement directory for paper\n",
    "    doublet_df.to_csv(os.path.join(supplement_table_dir, 'Table19_Doublet_Rates.csv'), index=False)\n",
    "    \n",
    "else:\n",
    "    ##Filtered and Processed Data objects\n",
    "    filtered_doublet_files = {\n",
    "     '10X_3-rep1':  ##INSERT HERE,\n",
    "     '10X_3-rep2':  ##INSERT HERE,\n",
    "     '10X_5-rep1':  ##INSERT HERE,\n",
    "     '10X_5-rep2':  ##INSERT HERE,\n",
    "     '10X_FRP-rep1':  ##INSERT HERE,\n",
    "     '10X_FRP-rep2':  ##INSERT HERE,\n",
    "     'BD-rep1':  ##INSERT HERE,\n",
    "     'BD-rep2':  ##INSERT HERE,\n",
    "     'Fluent-rep1':  ##INSERT HERE,\n",
    "     'Fluent-rep2':  ##INSERT HERE,\n",
    "     'Fluent-rep3':  ##INSERT HERE,\n",
    "     'Honeycomb-rep1':  ##INSERT HERE,\n",
    "     'Honeycomb-rep2':  ##INSERT HERE,\n",
    "     'Parse-rep1':  ##INSERT HERE,\n",
    "     'Scale-rep1':  ##INSERT HERE,\n",
    "     'Scipio-rep1':  ##INSERT HERE,\n",
    "     'Scipio-rep2':  ##INSERT HERE,\n",
    "     'Singleron-rep1':  ##INSERT HERE,\n",
    "     'Singleron-rep2':  ##INSERT HERE,\n",
    "     'Broad-Reference-Val':  ##INSERT HERE,\n",
    "     'Broad-Reference': ##INSERT HERE\n",
    "    }\n",
    "    \n",
    "    filtered_doublet_files = {method:os.path.join(anndata_object_dir, file) for method, file in filtered_doublet_files.items()}\n",
    "    \n",
    "    ##load\n",
    "    adata_all = {}\n",
    "    for method, file in filtered_doublet_files.items():\n",
    "        \n",
    "        ## load adata files\n",
    "        print(f\"Loading {method} ...\")\n",
    "        adata_all[method] = sc.read_h5ad(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    doublet_df = pd.read_csv(os.path.join(doublet_dir, 'doublet_summary_df.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ea866d-34e1-443c-986d-07e155683566",
   "metadata": {},
   "source": [
    "Now we can subsample to the smallest size from all of them (excluding scipio, since it can only do 5000 cells max), and refilter for genes and cells that don't meet our cutoff (genes need to be seen in at least 3 cells and cells need to have at least 5 genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9498a0-e87a-4a95-b3ca-a1e782e38965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 10X_3-rep1 ...\n",
      "Loading 10X_3-rep2 ...\n",
      "Loading 10X_5-rep1 ...\n",
      "Loading 10X_5-rep2 ...\n",
      "Loading 10X_FRP-rep1 ...\n",
      "Loading 10X_FRP-rep2 ...\n",
      "Loading BD-rep1 ...\n",
      "Loading BD-rep2 ...\n",
      "Loading Fluent-rep1 ...\n",
      "Loading Fluent-rep2 ...\n",
      "Loading Fluent-rep3 ...\n",
      "Loading Honeycomb-rep1 ...\n",
      "Loading Honeycomb-rep2 ...\n",
      "Loading Parse-rep2 ...\n",
      "Loading Scale-rep1 ...\n",
      "Loading Scipio-rep1 ...\n",
      "Loading Scipio-rep2 ...\n",
      "Loading Singleron-rep1 ...\n",
      "Loading Singleron-rep2 ...\n",
      "Loading Broad-Reference-Val ...\n",
      "Loading Broad-Reference ...\n"
     ]
    }
   ],
   "source": [
    "if filter_cells:\n",
    "    \n",
    "    ##downsample size (min cells in experiments excluding scipio since it can only process about 5000 cells)\n",
    "    filter1_sizes = {f'{method}':data.shape[0] for method, data in adata_all.items() if 'scipio' not in method.lower()}\n",
    "    n_downsample = min(filter1_sizes.values())\n",
    "    print(f'Lowest Sample Size ({min(filter1_sizes)}: {n_downsample})')\n",
    "\n",
    "    ##set seed for downsampling\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    ##initialize data lists and dicts\n",
    "    selected_barcodes={} ##downsampled barcodes that we can reuse (since we've tagged cell barcodes with method tag)\n",
    "\n",
    "    ##Loop through adata dictionary to filter and downsample\n",
    "    for method, adata in adata_all.items():\n",
    "\n",
    "        ##copy adata to prevent in place modifications\n",
    "        adata = adata.copy()\n",
    "\n",
    "        ##filter barcodes with few genes genes or counts since they are not useful for this method\n",
    "        ##min_genes and min_cells already set (5 and 4, respectively, but we will do this step again)\n",
    "        sc.pp.filter_cells(adata, min_genes = min_genes) ##5 is default because used in scATAQ-seq paper\n",
    "\n",
    "        ## downsample to lowest cell number (EXCEPT FOR REFERENCE AND SCIPIO; don't want to lose training data or excess cells due to scipio)\n",
    "        if method not in ['Broad-Reference', 'Scipio-rep1', 'Scipio-rep2']:\n",
    "            sel_barcodes = np.random.choice(adata.obs_names, n_downsample, replace=False)\n",
    "            selected_barcodes[method]=sel_barcodes\n",
    "            adata = adata[sel_barcodes]\n",
    "        else: ##if scipio, use all barcodes since the platform can only handle about 5000 cells max \n",
    "            selected_barcodes[method]=adata.obs_names\n",
    "\n",
    "        ## filter genes not in at least min_cells genes (filter AFTER downsample to make sure we check all available cells)\n",
    "        sc.pp.filter_genes(adata, min_cells = min_cells) ##3 is default because used in scATAQ-seq paper\n",
    "\n",
    "        ##subset reference and validation metadata for annotation later\n",
    "        if method == 'Broad-Reference':\n",
    "            reference_metadata = adata.obs\n",
    "        elif method == 'Broad-Reference-Val':\n",
    "            validation_metadata = adata.obs ##save metadata for evaluation metrics\n",
    "        \n",
    "    \n",
    "        ##Add .obs column for the Broad-Reference batch_key\n",
    "        if method not in ['Broad-Reference', 'Broad-Reference-Val']:\n",
    "            adata.obs['platform_broadincluded'] = adata.obs['platform']\n",
    "        else:\n",
    "            adata.obs['platform_broadincluded'] = [f\"{platform}_{broad}\" for platform, broad in zip(adata.obs['platform'], adata.obs['Method'])]\n",
    "\n",
    "        ##update raw with newly filtered info (still not normalized X) also helps overcome Broad-Reference data save\n",
    "        adata.raw = adata.copy()\n",
    "\n",
    "        print(f\"Method: {method}\")\n",
    "\n",
    "        output_file = os.path.join(anndata_object_dir, f'{method}_filtered_nodoublets.h5ad')\n",
    "        adata.write_h5ad(output_file)\n",
    "        \n",
    "        ##update adata with downsampled data\n",
    "        adata_all[method] = adata\n",
    "        \n",
    "        ##delete for memory\n",
    "        del adata\n",
    "        \n",
    "else:\n",
    "    \n",
    "    ##Filtered and Processed Data objects\n",
    "    filtered_files =  {\n",
    "     '10X_3-rep1':  ##INSERT HERE,\n",
    "     '10X_3-rep2':  ##INSERT HERE,\n",
    "     '10X_5-rep1':  ##INSERT HERE,\n",
    "     '10X_5-rep2':  ##INSERT HERE,\n",
    "     '10X_FRP-rep1':  ##INSERT HERE,\n",
    "     '10X_FRP-rep2':  ##INSERT HERE,\n",
    "     'BD-rep1':  ##INSERT HERE,\n",
    "     'BD-rep2':  ##INSERT HERE,\n",
    "     'Fluent-rep1':  ##INSERT HERE,\n",
    "     'Fluent-rep2':  ##INSERT HERE,\n",
    "     'Fluent-rep3':  ##INSERT HERE,\n",
    "     'Honeycomb-rep1':  ##INSERT HERE,\n",
    "     'Honeycomb-rep2':  ##INSERT HERE,\n",
    "     'Parse-rep1':  ##INSERT HERE,\n",
    "     'Scale-rep1':  ##INSERT HERE,\n",
    "     'Scipio-rep1':  ##INSERT HERE,\n",
    "     'Scipio-rep2':  ##INSERT HERE,\n",
    "     'Singleron-rep1':  ##INSERT HERE,\n",
    "     'Singleron-rep2':  ##INSERT HERE,\n",
    "     'Broad-Reference-Val':  ##INSERT HERE,\n",
    "     'Broad-Reference': ##INSERT HERE\n",
    "    }\n",
    "    \n",
    "    filtered_files = {method:os.path.join(anndata_object_dir, file) for method, file in filtered_files.items()}\n",
    "    \n",
    "    ##initialize data lists and dicts\n",
    "    adata_all = {}\n",
    "    selected_barcodes={} ##downsampled barcodes that we can reuse (since we've tagged cell barcodes with method tag)\n",
    "    \n",
    "    for method, file in filtered_files.items():\n",
    "        \n",
    "        ## load adata files\n",
    "        print(f\"Loading {method} ...\")\n",
    "        adata_all[method] = sc.read_h5ad(file)\n",
    "        \n",
    "        ## extract filtered and downsampled barcodes\n",
    "        selected_barcodes[method] = adata_all[method].obs_names\n",
    "        \n",
    "        ##subset reference and validation metadata for annotation later\n",
    "        if method == 'Broad-Reference':\n",
    "            reference_metadata = adata_all[method].obs\n",
    "        elif method == 'Broad-Reference-Val':\n",
    "            validation_metadata = adata_all[method].obs ##save metadata for evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c98307-9afa-422d-880e-7d57dfdb3d16",
   "metadata": {},
   "source": [
    "Show doublet rate for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c1d15d-0381-4f96-a312-6e54c05ef26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10X_3-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.03288773216609488, 'n_doublet': 278}\n",
      "\n",
      "____________________________________________________________\n",
      "10X_3-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.027156549520766772, 'n_doublet': 306}\n",
      "\n",
      "____________________________________________________________\n",
      "10X_5-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.03287995269071555, 'n_doublet': 278}\n",
      "\n",
      "____________________________________________________________\n",
      "10X_5-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.02876459741856177, 'n_doublet': 234}\n",
      "\n",
      "____________________________________________________________\n",
      "10X_FRP-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.03654517254749903, 'n_doublet': 377}\n",
      "\n",
      "____________________________________________________________\n",
      "10X_FRP-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.03702783300198807, 'n_doublet': 298}\n",
      "\n",
      "____________________________________________________________\n",
      "BD-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.02417718168181417, 'n_doublet': 274}\n",
      "\n",
      "____________________________________________________________\n",
      "BD-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.0331928032777151, 'n_doublet': 559}\n",
      "\n",
      "____________________________________________________________\n",
      "Fluent-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.008294361598870556, 'n_doublet': 94}\n",
      "\n",
      "____________________________________________________________\n",
      "Fluent-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.007665437609159713, 'n_doublet': 79}\n",
      "\n",
      "____________________________________________________________\n",
      "Fluent-rep3 Doublet Stats:\n",
      "{'doublet_rate': 0.005302695536897923, 'n_doublet': 48}\n",
      "\n",
      "____________________________________________________________\n",
      "Honeycomb-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.0316031603160316, 'n_doublet': 316}\n",
      "\n",
      "____________________________________________________________\n",
      "Honeycomb-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.0247, 'n_doublet': 247}\n",
      "\n",
      "____________________________________________________________\n",
      "Parse-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.02855309592556946, 'n_doublet': 445}\n",
      "\n",
      "____________________________________________________________\n",
      "Scale-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.034611553784860555, 'n_doublet': 417}\n",
      "\n",
      "____________________________________________________________\n",
      "Scipio-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.02095723591050694, 'n_doublet': 74}\n",
      "\n",
      "____________________________________________________________\n",
      "Scipio-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.02917946467749013, 'n_doublet': 133}\n",
      "\n",
      "____________________________________________________________\n",
      "Singleron-rep1 Doublet Stats:\n",
      "{'doublet_rate': 0.016777941559004597, 'n_doublet': 387}\n",
      "\n",
      "____________________________________________________________\n",
      "Singleron-rep2 Doublet Stats:\n",
      "{'doublet_rate': 0.021761016030357006, 'n_doublet': 562}\n",
      "\n",
      "____________________________________________________________\n",
      "Broad-Reference-Val Doublet Stats:\n",
      "{'doublet_rate': 0.0112, 'n_doublet': 112}\n",
      "\n",
      "____________________________________________________________\n",
      "Broad-Reference Doublet Stats:\n",
      "{'doublet_rate': 0.008610437181865754, 'n_doublet': 181}\n",
      "\n",
      "____________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for method, adata in adata_all.items():\n",
    "    print(f\"{method} Doublet Stats:\")\n",
    "    print(f\"{adata.uns['stats']}\")\n",
    "    print(\"\")\n",
    "    print(\"____________________________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8a83f85-7ac2-4822-888a-957234fe6245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>n_doublets</th>\n",
       "      <th>doublet_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10X_3-rep1</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.032888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10X_3-rep2</td>\n",
       "      <td>306.0</td>\n",
       "      <td>0.027157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10X_5-rep1</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.032880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10X_5-rep2</td>\n",
       "      <td>234.0</td>\n",
       "      <td>0.028765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10X_FRP-rep1</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.036545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10X_FRP-rep2</td>\n",
       "      <td>298.0</td>\n",
       "      <td>0.037028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BD-rep1</td>\n",
       "      <td>274.0</td>\n",
       "      <td>0.024177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BD-rep2</td>\n",
       "      <td>559.0</td>\n",
       "      <td>0.033193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Fluent-rep1</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.008294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fluent-rep2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.007665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fluent-rep3</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.005303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Honeycomb-rep1</td>\n",
       "      <td>316.0</td>\n",
       "      <td>0.031603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Honeycomb-rep2</td>\n",
       "      <td>247.0</td>\n",
       "      <td>0.024700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Singleron-rep1</td>\n",
       "      <td>387.0</td>\n",
       "      <td>0.016778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Singleron-rep2</td>\n",
       "      <td>562.0</td>\n",
       "      <td>0.021761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Scipio-rep1</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.020957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Scipio-rep2</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.029179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Parse-rep2</td>\n",
       "      <td>445.0</td>\n",
       "      <td>0.028553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Scale-rep1</td>\n",
       "      <td>417.0</td>\n",
       "      <td>0.034612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Broad-Reference-Val</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Broad-Reference</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.008610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 method  n_doublets  doublet_rate\n",
       "0            10X_3-rep1       278.0      0.032888\n",
       "1            10X_3-rep2       306.0      0.027157\n",
       "2            10X_5-rep1       278.0      0.032880\n",
       "3            10X_5-rep2       234.0      0.028765\n",
       "4          10X_FRP-rep1       377.0      0.036545\n",
       "5          10X_FRP-rep2       298.0      0.037028\n",
       "6               BD-rep1       274.0      0.024177\n",
       "7               BD-rep2       559.0      0.033193\n",
       "8           Fluent-rep1        94.0      0.008294\n",
       "9           Fluent-rep2        79.0      0.007665\n",
       "10          Fluent-rep3        48.0      0.005303\n",
       "11       Honeycomb-rep1       316.0      0.031603\n",
       "12       Honeycomb-rep2       247.0      0.024700\n",
       "13       Singleron-rep1       387.0      0.016778\n",
       "14       Singleron-rep2       562.0      0.021761\n",
       "15          Scipio-rep1        74.0      0.020957\n",
       "16          Scipio-rep2       133.0      0.029179\n",
       "17           Parse-rep2       445.0      0.028553\n",
       "18           Scale-rep1       417.0      0.034612\n",
       "19  Broad-Reference-Val       112.0      0.011200\n",
       "20      Broad-Reference       181.0      0.008610"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doublet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b16ffd5-137c-4759-91f3-031d8e1c774f",
   "metadata": {},
   "source": [
    "Check to make sure all downsampled sizes are the same except scipio and reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699c150e-74df-4b1e-9a24-0306a7781b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10X_3-rep1: 7750\n",
      "10X_3-rep2: 7750\n",
      "10X_5-rep1: 7750\n",
      "10X_5-rep2: 7750\n",
      "10X_FRP-rep1: 7750\n",
      "10X_FRP-rep2: 7750\n",
      "BD-rep1: 7750\n",
      "BD-rep2: 7750\n",
      "Fluent-rep1: 7750\n",
      "Fluent-rep2: 7750\n",
      "Fluent-rep3: 7750\n",
      "Honeycomb-rep1: 7750\n",
      "Honeycomb-rep2: 7750\n",
      "Singleron-rep1: 7750\n",
      "Singleron-rep2: 7750\n",
      "Scipio-rep1: 3457\n",
      "Scipio-rep2: 4425\n",
      "Parse-rep2: 7750\n",
      "Scale-rep1: 7750\n",
      "Broad-Reference: 20840\n",
      "Broad-Reference-Val: 7750\n"
     ]
    }
   ],
   "source": [
    "for k, v in adata_all.items():\n",
    "    print(f'{k}: {v[v.obs[\"platform\"] == k].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040340e9-3cc8-492a-a15c-bed408a070fc",
   "metadata": {},
   "source": [
    "Add .obs column for the Broad-Reference batch_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae8d36-9982-4ab3-b1cf-c5e17b15d273",
   "metadata": {},
   "source": [
    "Save selected barcodes as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b98a517-8676-4dcd-8f22-2d507264fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##convert to selected barcodes dictionary to list of tuples (platform, barcode) for each barcode\n",
    "##Then convert this to dataframe using the first tuple element as the platform and the second as the tagged barcode\n",
    "selected_barcodes_df = pd.DataFrame([(platform, barcode) for platform, barcodes in selected_barcodes.items() for barcode in barcodes], \n",
    "                                    columns=['platform', 'selected_barcodes_tagged'])\n",
    "\n",
    "# extract original barcodes using regex\n",
    "selected_barcodes_df['original_barcodes'] =  selected_barcodes_df.apply(lambda row: row.selected_barcodes_tagged.rsplit('_'+row.platform, 1)[0], axis=1)\n",
    "\n",
    "##save selected barcodes as CSV with original and new barcodes included\n",
    "anndata_object_dir = os.path.join(parent_dir, 'anndata_objects')\n",
    "os.makedirs(anndata_object_dir, exist_ok=True)\n",
    "barcode_output_file = os.path.join(anndata_object_dir, 'selected_barcode_per_platform_30k.csv')\n",
    "print(f\"Saving selected barcodes to {barcode_output_file}\")\n",
    "selected_barcodes_df.to_csv(barcode_output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0889477-6b5a-4318-8822-9409422b6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindDeviantGenes_singlebatch(adata, layer='unnormalized', top_pct=0.15, top_n=None, verbose=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    adata: AnnData object \n",
    "    layer: Layer of adata object to use for finding deviant genes. Defaults to 'unnormalized' since deviance should only be calculated on unnormalized data \n",
    "    top_pct: percent of top deviant genes to select. Defaults to 15%\n",
    "    top_n: number of top deviant genes to select (instead of top_pct; defaults to None, but if you set top_n, it overrides pct)\n",
    "    \n",
    "    Output:\n",
    "    adata object with highly deviant genes identifed and set as highly variable genes so that we can use only this subset\n",
    "    for pca\n",
    "    \"\"\"\n",
    "    \n",
    "    ##replace adata with a copy so we don't write in place\n",
    "    adata = adata.copy()\n",
    "    \n",
    "    ##check if top_pct or top_n (if top_n provided, it overrides top_pct)\n",
    "    if top_n is not None:\n",
    "        top_pct = None\n",
    "        if verbose is True: print(f'Detecting top {top_n} deviant genes deviant features ...\\n')\n",
    "    elif top_pct is not None:\n",
    "        if verbose is True: print(f'Detecting top {top_pct*100}% deviant genes deviant features ...\\n')\n",
    "    else:\n",
    "        raise ValueError(\"Either top_pct or top_n must be provided; both cannot be None.\")\n",
    "    \n",
    "    if layer != 'unnormalized':\n",
    "        if verbose is True: print(\"WARNING: Gene expression matrix used for deviant gene detection should be unnormalized counts\")\n",
    "        \n",
    "    elif hasattr(adata, 'layers') and adata.layers.get('unnormalized') is None:\n",
    "        raise ValueError(\"\"\"\n",
    "        Anndata object must either have an 'unnormalized' layer \n",
    "        OR you must specify the layer to use for deviant gene detection (specifying 'None' will default to adata.X).\n",
    "        Note that the gene expression matrix used for deviant gene detection should be unnormalized/raw counts\n",
    "        \"\"\")\n",
    "        \n",
    "    ##Subset the layer we will use (defaults to unnormalized)\n",
    "    adata_X = adata.layers[layer] if layer is not None else adata.X\n",
    "     \n",
    "    ##detect deviance based on top_n\n",
    "    if top_n is not None:\n",
    "        ##identify highly deviant genes (top N) using a binomial deviance calculation\n",
    "        deviance = hdg.highly_deviant_genes(X = adata_X, top_n = top_n, \n",
    "                                            family = 'binomial', gene_names =adata.var.index.to_list())\n",
    "\n",
    "    ##detect deviance based on top_pct\n",
    "    else:\n",
    "        ##identify highly deviant genes (top %) using a binomial deviance calculation\n",
    "        deviance = hdg.highly_deviant_genes(X = adata_X, top_pct = top_pct, \n",
    "                                            family = 'binomial', gene_names =adata.var.index.to_list())\n",
    "        \n",
    "    del adata_X\n",
    "\n",
    "    ##Finish deviance processing\n",
    "    adata.var['singlebatch_deviance'] = deviance['deviance'] ##get deviance scores\n",
    "    adata.var['highly_deviant'] = deviance['highly_deviant'] ##get deviance labels\n",
    "\n",
    "    ##Total unfiltered genes and selected genes\n",
    "    n_deviant = adata.var['highly_deviant'].sum()\n",
    "    if verbose is True: print(f\"Total features in data: {len(adata.var)}\")\n",
    "\n",
    "    ##print for number of deviant genes\n",
    "    if verbose is True:\n",
    "        if top_n is not None:\n",
    "            print(f'Selected top {top_n} deviant genes')\n",
    "        else: \n",
    "            print(f\"Top {top_pct*100}% deviant genes feature number: {n_deviant}\")\n",
    "\n",
    "    ## set highly_variable as highly_deviant so we can use in PCA\n",
    "    adata.var[\"highly_variable\"] = adata.var[\"highly_deviant\"]\n",
    "\n",
    "    ##subset the data to highly deviant genes, rank by deviance, and create results dictionary\n",
    "    deviant_df = adata.var[adata.var['highly_deviant'] == True][['singlebatch_deviance']] ##subset highly deviant genes and scores\n",
    "    deviant_df = deviant_df.reset_index().rename(columns={'index':'gene_names'}) ##turn index to a column for gene names\n",
    "    deviant_df.sort_values(by='singlebatch_deviance', ascending=False, inplace=True) ##rank according to deviance scores\n",
    "    deviant_df['rank'] = range(1, len(deviant_df) + 1) ##give a rank\n",
    "\n",
    "    ##add stats and res back to adata\n",
    "    adata.uns['deviant_genes'] = deviant_df\n",
    "    return adata\n",
    "\n",
    "def FindDeviantGenes(adata, layer='unnormalized', top_pct=0.15, top_n=None, batch_key=None, min_cells=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    adata: AnnData object \n",
    "    layer: Layer of adata object to use for finding deviant genes. Defaults to 'unnormalized' since deviance should only be calculated on unnormalized data \n",
    "    top_pct: percent of top deviant genes to select. Defaults to 15%\n",
    "    top_n: number of top deviant genes to select (instead of top_pct; defaults to None, but if you set top_n, it overrides pct)\n",
    "    batch_key: string indicating adata.obs column indicating sample batches \n",
    "               (if supplied, returns top highly deviant genes of intersection of genes across batches)\n",
    "    min_cells: The minimum number of cells that a gene must be in for a subset (within batch) to count\n",
    "    \n",
    "    Output:\n",
    "    adata object with highly deviant genes identifed and set as highly variable genes so that we can use only this subset\n",
    "    for pca\n",
    "    \"\"\"\n",
    "    \n",
    "    ##replace adata with a copy so we don't write in place\n",
    "    adata = adata.copy()\n",
    "        \n",
    "    ##Process per batch if batch_key is provided\n",
    "    if batch_key is not None:\n",
    "        \n",
    "        ##give warniing if layer is not unnormalized\n",
    "        if layer != 'unnormalized':\n",
    "            print(\"WARNING: Gene expression matrix used for deviant gene detection should be unnormalized counts\")\n",
    "            \n",
    "        batches = np.unique(adata.obs[batch_key])\n",
    "        batch_values = adata.obs[batch_key].values\n",
    "        \n",
    "        adata_dict = {}\n",
    "        for batch in batches:\n",
    "            \n",
    "            ##subset batch\n",
    "            adata_batch = adata[batch_values == batch]\n",
    "            \n",
    "            ##since the data is joined, the subsets might have genes with no counts, so we must remove those for the deviance calc to work\n",
    "            sc.pp.filter_genes(adata_batch, min_cells = min_cells) ##3 is default because used in scATAQ-seq paper\n",
    "            \n",
    "            ##get deviance per batch\n",
    "            adata_dict[batch] = FindDeviantGenes_singlebatch(adata_batch, layer=layer, top_pct=top_pct, top_n=top_n, verbose=False)\n",
    "            \n",
    "            del adata_batch\n",
    "            \n",
    "        ##Rank genes by how many batches they were identified as deviant in and select top_perc or top_N\n",
    "        ##Note that we only do this on those genes that are present in EACH batch (Intersection of Genes)\n",
    "        ##Ties are broken by the median deviance across all batches\n",
    "        \n",
    "        ## Calculate the intersection of indices\n",
    "        gene_intersect = set.intersection(*map(set, [a.var_names for a in adata_dict.values()]))\n",
    "        \n",
    "        ## Identify number of genes we need\n",
    "        if top_n is not None:\n",
    "            top_pct = None\n",
    "            n_deviant = top_n\n",
    "            print(f'Detecting top {top_n} deviant genes deviant features ...\\n')\n",
    "        elif top_pct is not None:\n",
    "            print(f'Detecting top {top_pct*100}% deviant genes deviant features (from the intersection) ...\\n')\n",
    "            n_deviant = int(np.round(top_pct * len(gene_intersect))) ##n genes that will give top_pct of genes\n",
    "        else:\n",
    "            raise ValueError(\"Either top_pct or top_n must be provided; both cannot be None.\")\n",
    "            \n",
    "        ##print some messages   \n",
    "        print(f\"Total features in data: {len(adata.var)}\")\n",
    "        print(f\"Total intersecting features in data: {len(gene_intersect)}\")\n",
    "\n",
    "        ## Subset datasets to intersecting genes while preserving original order\n",
    "        subset_datasets = []\n",
    "        for batch_name, a in adata_dict.items():\n",
    "            subset_indices = [index for index in a.var_names if index in gene_intersect]\n",
    "            subset_data = a.var.loc[subset_indices]\n",
    "            subset_datasets.append(subset_data[['singlebatch_deviance', 'highly_deviant']].to_numpy()) ##select only deviance (number) and highly_deviant (T/F) cols\n",
    "            del subset_data\n",
    "            \n",
    "        del adata_dict\n",
    "\n",
    "        ##Create a df containing median deviance and number of platforms where that gene was highly deviant across platforms\n",
    "        batch_deviance_df = pd.DataFrame({'median_deviance': np.median(np.dstack(subset_datasets)[:,0,:], axis = 1),\n",
    "                                          'n_platforms_deviant': np.dstack(subset_datasets)[:,1,:].sum(axis = 1)\n",
    "                                         })\n",
    "        batch_deviance_df.index = subset_indices ##set index as the gene names from the subset (intersection)\n",
    "        \n",
    "        ##Sort values on median deviance first (so we can break ties by median_deviance) and then rank on # platforms where it was highly_deviant\n",
    "        batch_deviance_df = batch_deviance_df.sort_values(by = 'median_deviance', ascending = False)\n",
    "        batch_deviance_df['deviant_gene_rank']=batch_deviance_df['n_platforms_deviant'].rank(method ='first', ascending = False)\n",
    "        batch_deviance_df['highly_deviant'] = batch_deviance_df['deviant_gene_rank'] <= n_deviant\n",
    "        batch_deviance_df = batch_deviance_df.loc[subset_indices]\n",
    "        \n",
    "        ##update adata object with identified deviant genes (set highly variable as highly deviant for PCA)\n",
    "        ##Make sure we update the correct indives by subsetting batch_deviance.loc[subset_indices] again even though it is redundant\n",
    "        adata.var['median_deviance'] = np.nan\n",
    "        adata.var['deviant_gene_rank'] = np.nan\n",
    "        adata.var['n_platforms_deviant'] = np.nan\n",
    "        adata.var['highly_deviant'] = False\n",
    "        adata.var.loc[subset_indices, 'median_deviance'] = batch_deviance_df.loc[subset_indices, 'median_deviance']\n",
    "        adata.var.loc[subset_indices, 'deviant_gene_rank'] = batch_deviance_df.loc[subset_indices, 'deviant_gene_rank']\n",
    "        adata.var.loc[subset_indices, 'n_platforms_deviant'] = batch_deviance_df.loc[subset_indices, 'n_platforms_deviant']\n",
    "        adata.var.loc[subset_indices, 'highly_deviant'] = batch_deviance_df.loc[subset_indices, 'highly_deviant']\n",
    "        adata.var['highly_variable'] = adata.var['highly_deviant']\n",
    "        \n",
    "        ##Explicitly convert median_deviance and n_platforms_deviant to float objects so that we can save to hda5 (otherwise error)\n",
    "        adata.var['median_deviance'] = adata.var['median_deviance'].astype(np.float64)\n",
    "        adata.var['n_platforms_deviant'] = adata.var['n_platforms_deviant'].astype(np.float64)\n",
    "        \n",
    "        ##add deviant gene df to adata.uns\n",
    "        # adata.uns['deviant_genes'] = batch_deviance_df\n",
    "        \n",
    "        ##print for number of deviant genes\n",
    "        if top_n is not None:\n",
    "            print(f'Selected top {top_n} deviant genes')\n",
    "        else: \n",
    "            print(f\"Top {top_pct*100}% deviant genes feature number: {n_deviant}\")\n",
    "            \n",
    "        del batch_deviance_df\n",
    "            \n",
    "        return adata\n",
    "            \n",
    "    else:\n",
    "        adata = FindDeviantGenes_singlebatch(adata, layer=layer, top_pct=top_pct, top_n=top_n)\n",
    "\n",
    "        return adata\n",
    "\n",
    "\n",
    "def plot_PCA(adata, pca_basis='X_pca', hue=None, figsize = (8,6), title=\"\", titlesize=12, markersize = 10, colors=None, bbox_to_anchor=(1.3,1.01)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    adata: AnnData object (must have PCA already performed)\n",
    "    pca_basis: Key for PCA data in adata.obsm\n",
    "    hue: Variable to group by for plotting (Defaults to None)\n",
    "    figsize: Desired size of figure (tuple; Defaults to (8,6))\n",
    "    title: Title to give plot\n",
    "    titlesize: Font to give title (defaults to 12)\n",
    "    markersize: Size for points on scatter (defaults to 10)\n",
    "    colors: List of colors to give to all groups from hue (Defaults to None)\n",
    "    \n",
    "    Output:\n",
    "    PCA plot of provided data grouped by hue (if provided)\n",
    "    \"\"\"\n",
    "    \n",
    "    ##ONLY necessary for coloring by platform. scanpy gives an error for some reason\n",
    "    \n",
    "    ##Get obs variables from data\n",
    "    scatter_df = adata.obs.copy()\n",
    "    \n",
    "    ##add PC data\n",
    "    scatter_df = scatter_df.assign(PC1 = adata.obsm[pca_basis][:,0],\n",
    "                                   PC2 = adata.obsm[pca_basis][:,1])\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    \n",
    "    if hue is not None:\n",
    "        if colors is not None:\n",
    "            sns.scatterplot(data = scatter_df, x = \"PC1\", y = \"PC2\", hue = hue, palette = colors, ax = ax, s = markersize)\n",
    "        else:\n",
    "            sns.scatterplot(data = scatter_df, x = \"PC1\", y = \"PC2\", hue = hue, ax = ax, s = markersize)\n",
    "            \n",
    "        ax.legend(title = hue, loc='upper right', bbox_to_anchor=bbox_to_anchor, prop={'size': 9})\n",
    "    else:\n",
    "        sns.scatterplot(data = scatter_df, x = \"PC1\", y = \"PC2\", ax = ax)\n",
    "    \n",
    "    ## remove axis ticks\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "    \n",
    "    ##Set title\n",
    "    ax.set_title(title, fontsize=titlesize)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    del scatter_df\n",
    "        \n",
    "def log1p_normalize(adata):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input: adata object with counts data stored in adata.X (should be unnormalized)\n",
    "    \n",
    "    Output: adata object with log1p(size corrected counts) stored in x and unnormalized counts \n",
    "            stored in adata.layers['unnormalized']\n",
    "            \n",
    "            \n",
    "    Theory:\n",
    "    \n",
    "    The counts we have in our data are generated after cell capture, reverse transcription, amplification, and sequencing. \n",
    "    These steps inherently vary per cell, so the counts we see represent not only the biological variation per cell, but\n",
    "    the technical variation as well. \n",
    "    \n",
    "    Normalizing data is a valuable preprocessing step to adjust these counts in the dataset for technical variance by \n",
    "    scaling the observed variance into a specific range. There are many techniques (log shifted transformation, pearson r\n",
    "    esiduals, etc.) used to make subsequent analysis and statistics applicable, but their usage depends on the situation at \n",
    "    hand.\n",
    "    \n",
    "    According to the Theis et. al's Single Cell Best Practices book, the shifted logarithm approach is useful for stabilizing\n",
    "    variance and identifying differentially expressed genes while the pearson residual approach is useful for identifying biologically \n",
    "    relevant genes and rare cell types. \n",
    "    \n",
    "    A recent benchmark by [Ahlmann-Eltze & Huber (2023)](https://doi.org/10.1038/s41592-023-01814-1)  \n",
    "    revealed that the shifted logarithm approach,  demonstrates superior performance compared to other methods in\n",
    "    uncovering underlying latent structures and stabilizing variance for identifying differentially expressed genes, \n",
    "    especially when followed by PCA. In this approach, the log-shifted counts are defined as:\n",
    "    \n",
    "   We will use the log-shifted normalization method because it performs well in the benchmark study and works to \n",
    "   identify differentially expressed genes. However, we will use the default size factor scaling\n",
    "   provided in scanpy's `sc.pp.normalize_total()` function instead of the average counts as in the Ahlmann-Eltze & Huber\n",
    "   paper. scanpy's default size factor scale is the median count across all cells, which should produce similar results \n",
    "   to the Ahlmann-Eltze & Huber paper (the only difference is that we use median instead of mean counts). This is performed\n",
    "   below. \n",
    "   \n",
    "   Note that we will not set a target_sum in `sc.pp.normalize_total` because we don't want to define a set scaling factor \n",
    "   for the data (ie $L=10^6$ would give us counts per million).\n",
    "   \"\"\"\n",
    "    \n",
    "    ## Normalizing Data\n",
    "\n",
    "    ##recalculate metrics to get total_counts before normalizing\n",
    "    sc.pp.calculate_qc_metrics(adata, log1p = True, percent_top=[20], inplace=True)\n",
    "\n",
    "    ##check if data has already been normalized (will have an 'unnormalized' layer)\n",
    "    if adata.layers.get('unnormalized') is None:\n",
    "\n",
    "        warnings.warn(\"Normalizing (and log1p) total counts from adata.X. Make sure adata.X is not already normalized, or it may cause problems.\")\n",
    "\n",
    "        #save unnormalized count data to adata.layers['unnormalized'] for feature selection using deviance\n",
    "        adata.layers['unnormalized'] = adata.X.copy()\n",
    "\n",
    "    else: ##ie it has already been normalized with log1p_normalize\n",
    "\n",
    "        warnings.warn(f\"Data already normalized. Using adata.layers['unnormalized'] to normalize and log1p data. Storing at adata.X\")\n",
    "        adata.X = adata.layers['unnormalized'].copy()\n",
    "\n",
    "    ##Either way, scale and log-shift transform data\n",
    "    scales_counts = sc.pp.normalize_total(adata, target_sum=None, inplace=False)##does the y/s_c calculation\n",
    "    adata.X = sc.pp.log1p(scales_counts[\"X\"], copy=True) ##does log-shift ie log(y_scaled + 1)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "def preprocessData(adata, scale=True, normalize_data=True, batch_preprocess=False, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    adata: AnnData object\n",
    "    normalize_data: Whether to log1p normalize data (Defaults to True because it is in Best Practices (See Below))\n",
    "    scale: Whether to scale data to mean of zero and variance of 1 (Defaults to True)\n",
    "    batch_preprocess: Whether to perform batch-specific normalization and scaling (Usually recommended except for integration with Harmony Based on our Results). Defaults to False\n",
    "    **kwargs: any keyword argument that can be supplied to internal functions\n",
    "    \n",
    "    Output:\n",
    "    adata object that has been log1p_normalized, had highly deviant genes identifed and set as highly variable genes, and been \n",
    "    scaled to a mean of zero and variance of 1 (if requested) so that we can use only this subset for pca\n",
    "    \n",
    "    Best Practices: https://www.sc-best-practices.org/preprocessing_visualization\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ###Extract potential kwargs (Default after comma)\n",
    "    layer = kwargs.get('layer', 'unnormalized') ##default is 'unnormalized'\n",
    "    top_pct = kwargs.get('top_pct', 0.15) ##default is 2000 for top deviance pct\n",
    "    top_n = kwargs.get('top_n', None) ##default is None for top deviance n since it defaults to using pct\n",
    "    batch_key = kwargs.get('batch_key', None) ##default is none (no batch correction; single batch processing)\n",
    "    min_cells = kwargs.get('min_cells', 3) ##default is 3 (only relevant for when batch_key is provided)\n",
    "    max_value = kwargs.get('max_value', None) ##Default is none for max_value (value to clip to when scaling)\n",
    "    \n",
    "    ##Message warning users to use raw counts for Deviant Detection\n",
    "    if (scale | normalize_data):\n",
    "        print(\"Normalizing and/or Scaling Data; Make sure to provided raw count layer for Deviance Detection\")\n",
    "    \n",
    "    ##Normalize (Get's metrics for total counts and then: sc.pp.normalize_total(adata, target_sum=None, inplace=False)##does the y/s_c calculation)\n",
    "    \n",
    "    ##check if batch preprocessing is desired to decide if we do per batch normalization, scaling\n",
    "    if batch_preprocess is False:\n",
    "        \n",
    "        ##If batch_key is None and batch_scale or batch_normalize are set as True, give error\n",
    "        if batch_key is not None:\n",
    "            print(\"'batch_preprocess' set to False but batch_key provided.\") \n",
    "            print(\"Only batch deviance detection will be performed not batch normalization and scaling.\")\n",
    "        \n",
    "        if normalize_data is True: \n",
    "            print(\"Normalizing Data ...\")\n",
    "            adata = log1p_normalize(adata)\n",
    "        \n",
    "        ##Scale data (defaults to mean of zero and sd of 1)\n",
    "        if scale is True:\n",
    "            print(\"Scaling Data ...\")\n",
    "            adata = sc.pp.scale(adata, max_value=max_value, copy = True) ##Not done inplace\n",
    "            \n",
    "        # check if data is sparse, if not, make it sparse (scaling makes data not sparse)\n",
    "        if not issparse(adata.X):\n",
    "            adata.X = csr_matrix(adata.X)\n",
    "\n",
    "        # check to make sure zeros are removed, if not, remove\n",
    "        if (adata.X.data == 0).any():\n",
    "            adata.X.eliminate_zeros()\n",
    "            \n",
    "      \n",
    "    ##if batch_preprocessing is desired, perform batch-specific scaling and normalization (if requested for each)\n",
    "    ##Note: User may choose to do batch_deviance detection but not batch scaling and normalization\n",
    "    else:\n",
    "        \n",
    "        ##If batch_key is None and batch_scale or batch_normalize are set as True, give error\n",
    "        if batch_key is None:\n",
    "            raise ValueError(\"'batch_preprocess' requires a batch_key variable to group by\")\n",
    "            \n",
    "        ##get batches\n",
    "        batches = np.unique(adata.obs[batch_key])\n",
    "        batch_values = adata.obs[batch_key].values\n",
    "        \n",
    "        \n",
    "        ##Print update messages\n",
    "        if normalize_data and scale:\n",
    "            print(\"Normalizing and Scaling Data per Batch ...\")\n",
    "        elif normalize_data:\n",
    "            print(\"Normalizing Data per Batch ...\")\n",
    "        elif scale:\n",
    "            print(\"Scaling Data per Batch ...\")\n",
    "        else:\n",
    "            print(\"Not Normalizing or Scaling Data ...\")\n",
    "                  \n",
    "\n",
    "        adata_dict = {}\n",
    "        for batch in batches:\n",
    "\n",
    "            ##subset batch\n",
    "            adata_batch = adata[batch_values == batch]\n",
    "\n",
    "            if normalize_data is True:\n",
    "\n",
    "                ##normalize per batch\n",
    "                adata_batch = log1p_normalize(adata_batch)\n",
    "\n",
    "            if scale is True:\n",
    "\n",
    "                ##Batch Scale Data (defaults to mean of zero and sd of 1)\n",
    "                adata_batch = sc.pp.scale(adata_batch, max_value=max_value, copy = True) ##Not done inplace\n",
    "                \n",
    "            # check if data is sparse, if not, make it sparse (scaling makes data not sparse)\n",
    "            if not issparse(adata_batch.X):\n",
    "                adata_batch.X = csr_matrix(adata_batch.X)\n",
    "\n",
    "            # check to make sure zeros are removed, if not, remove\n",
    "            if (adata_batch.X.data == 0).any():\n",
    "                adata_batch.X.eliminate_zeros()\n",
    "\n",
    "            ##add back to dict\n",
    "            adata_dict[batch] = adata_batch\n",
    "\n",
    "            del adata_batch\n",
    "\n",
    "        ##concatenate adata batch back into adata so we have batch normalized and scaled data to correct for differences in library size\n",
    "        adata = ad.concat(adata_dict, join = 'inner')\n",
    "\n",
    "        ##concatenation removes adata.var, so we will select the first adata.var since normalization will not change any of these\n",
    "        adata.var = adata_dict[batches[0]].var\n",
    "        \n",
    "        ##delete for memory\n",
    "        del adata_dict\n",
    "        gc.collect()\n",
    "            \n",
    "\n",
    "    ##Find Top Deviant Genes (Defaults to top 2000) correcting for batch_key if necessary \n",
    "    ##for deviance, only considers raw counts not normalized/scaled, so we can do after normalization and scaling\n",
    "    adata = FindDeviantGenes(adata, layer=layer, top_pct=top_pct, top_n=top_n, batch_key=batch_key, min_cells=min_cells)\n",
    "        \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214afb87-8497-4ce4-968a-14910fb22147",
   "metadata": {},
   "source": [
    "Now we can concatenate each platform with the reference to generate datasets unique to each platform for cell annotations and find the deviant genes that can be used for pca, umap, and annotation later. Note that for deviant gene selection, we are selecting top 2000 genes for all platforms so it is consistent. We will do a batch corrected deviant gene selection to identify commonly deviant genes. We will also do a query centric approach where we identify the top 2000 deviant genes in the query alone. This will emphasize platform-specific gene expression. Also note that we normalize to total and scale to a mean of 0 and variance of 1 for processing (does NOT impact deviant gene detection because it works on Unnormalized data). For normalization, we normalize per platform AND per method used to generate the Broad-Reference set (For instance within Broad we have inDrops, 10X, etc., so we will normalize each of these as their own batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e1c871e-ac45-4bca-90db-418791f553a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##concatenate each method with broad reference to generate a new adata object with reference for each\n",
    "\n",
    "##Initialize dictionary for saved files\n",
    "saved_files={}\n",
    "selected_batchcor_genes={}\n",
    "selected_common_genes={}\n",
    "selected_query_genes={}\n",
    "\n",
    "##Loop through adata to process\n",
    "for method, adata in adata_all.items():\n",
    "    \n",
    "    ##copy adata so we don't update in place\n",
    "    adata = adata.copy()\n",
    "    \n",
    "    ##don't combine Broad-Reference with itself\n",
    "    if method == \"Broad-Reference\":\n",
    "        continue \n",
    "    \n",
    "    ##concat with broad-ref\n",
    "    adata = ad.concat([adata, adata_all['Broad-Reference']], join='inner')\n",
    "    \n",
    "    ##set dummy variables for platforms (and convert to strings so plotting with scanpy works as binary (first bool then string))\n",
    "    sel_methods = np.unique(adata.obs['platform'])\n",
    "    adata.obs = pd.concat([adata.obs, pd.get_dummies(adata.obs['platform'], dtype=bool)], axis = 1) ##dummy variables for each platform for visualization\n",
    "    adata.obs[sel_methods] = adata.obs[sel_methods].replace({True: 'True', False: 'False'})\n",
    "\n",
    "    ## annotate mitochondrial (MT) and ribosomal (RPS and RPL) genes\n",
    "    adata.var['mt'] = adata.var_names.str.startswith('MT-')  # annotate mitochondrial gene group as 'mt'\n",
    "    adata.var['ribo'] = adata.var_names.str.startswith((\"RPS\",\"RPL\"))  # annotate ribosomal gene group as 'ribo'\n",
    "\n",
    "    ##Add relevant columns for cell-annotation later (only intersection)\n",
    "    adata.obs = adata.obs.assign(Cluster_baseline = \"Unassigned\")\n",
    "    adata.obs = adata.obs.assign(CellType_baseline = \"Unassigned\")\n",
    "    adata.obs.loc[adata.obs['platform'] == 'Broad-Reference', ['Cluster_baseline', 'CellType_baseline']] = reference_metadata[['Cluster', 'CellType']].values\n",
    "\n",
    "    if method == \"Broad-Reference-Val\":\n",
    "        adata.obs.loc[adata.obs['platform'] == 'Broad-Reference-Val', ['Cluster_baseline', 'CellType_baseline']] = validation_metadata[['Cluster', 'CellType']].values\n",
    "\n",
    "    ##Add a low resolution baseline (combine Monocytes, T Cells, and Dendritic Cells into one category, respectively)\n",
    "    adata.obs['CellType_baseline_lowres'] = adata.obs['CellType_baseline'].copy()\n",
    "    adata.obs['CellType_baseline_lowres'] = ['Monocyte (non-specific)' if 'monocyte' in cell.lower() else cell for cell in adata.obs['CellType_baseline_lowres']]\n",
    "    adata.obs['CellType_baseline_lowres'] = ['T Cell (non-specific)' if 't cell' in cell.lower() else cell for cell in adata.obs['CellType_baseline_lowres']]\n",
    "    adata.obs['CellType_baseline_lowres'] = ['Dendritic Cell (non-specific)' if 'dendritic' in cell.lower() else cell for cell in adata.obs['CellType_baseline_lowres']]\n",
    "\n",
    "    ## Before any further preprocessing make sure to set a .raw attribute and subset query data for query-specific deviant gene detection\n",
    "    adata.raw = adata\n",
    "    adata.layers['unnormalized'] = adata.raw.X\n",
    "    adata_query = adata[adata.obs['platform'] == method] ##subset query data so we can find query-specify deviation also\n",
    "    \n",
    "    ## Identify deviant genes for each platform (including Broad internal batches) separately so we can find commonly deviant genes (FEATURE_SELECT_GENE_N set at top of notebook)\n",
    "    ## This will also normalize per Broad-Reference internal batch\n",
    "    print(f\"Processing Data to Get Deviant Genes per Batch for {method}...\")\n",
    "    adata = preprocessData(adata, scale=True, normalize_data=True, batch_preprocess=True,\n",
    "                           layer='unnormalized', top_n=FEATURE_SELECT_GENE_N, batch_key='platform_broadincluded')\n",
    "    \n",
    "    ## Identify deviant genes for the query only so we can emphasize gene deviation in the query alone also\n",
    "    ## Here we also use a batch_key for Broad-Reference-Val, otherwise we use no batch key\n",
    "    print(f\"Processing Query to Get Deviant Genes per Query for {method}...\")\n",
    "    if method == 'Broad-Reference-Val':\n",
    "        print(\"(Processing Broad-Reference Batches Separately for Normalization and Deviant Gene Detection)\")\n",
    "        adata_query = preprocessData(adata_query, scale=True, normalize_data=True, batch_preprocess=True,\n",
    "                                     layer='unnormalized', top_n=FEATURE_SELECT_GENE_N, batch_key='platform_broadincluded')\n",
    "    else:\n",
    "        adata_query = preprocessData(adata_query, scale=True, normalize_data=True, batch_preprocess=False,\n",
    "                                     layer='unnormalized', top_n=FEATURE_SELECT_GENE_N) ##single batch method since only query\n",
    "    \n",
    "    ## Select those that are deviant in all platforms (including reference batch) as those to call deviant genes\n",
    "    dev_genes_batch = adata.var['n_platforms_deviant'] == adata.obs['platform_broadincluded'].nunique()\n",
    "    adata.var['highly_deviant_common'] = dev_genes_batch\n",
    "    \n",
    "    ## set those genes that are deviant in the query as those that are deviant/variable in the concatenated data\n",
    "    ## adata.var and adata_query.var should have same genes since adata_query was a subset from adata\n",
    "    if np.all(adata.var_names == adata_query.var_names):\n",
    "        adata.var['highly_deviant_query'] = adata_query.var['highly_deviant']\n",
    "    else:\n",
    "        raise ValueError(\"adata.var_names does not match adata_query.var_names\")\n",
    "        \n",
    "    ## delete for memory\n",
    "    del adata_query\n",
    "    \n",
    "    ##save genes\n",
    "    selected_common_genes[method] = adata.var_names[dev_genes_batch]\n",
    "    selected_batchcor_genes[method] = adata.var_names[adata.var['highly_deviant']]\n",
    "    selected_query_genes[method] = adata.var_names[adata.var['highly_deviant_query']]\n",
    "    \n",
    "    print(f\"Common Highly Deviant Genes for {method}: {np.sum(dev_genes_batch)}\")\n",
    "    print(f\"Highly Deviant Genes for {method}: {np.sum(adata.var['highly_deviant'])}\")\n",
    "    \n",
    "    ## Save updated adata objects to their own files\n",
    "    saved_files[method] = os.path.join(anndata_object_dir, f'{method}_intersection_w_reference_nodoublets.h5ad')\n",
    "    adata.write_h5ad(saved_files[method])\n",
    "    \n",
    "    ##delete for memory\n",
    "    del adata\n",
    "    \n",
    "    ##force garbage collection\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scanpy (Mamba)",
   "language": "python",
   "name": "scanpy-default-mamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
